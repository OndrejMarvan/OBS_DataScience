


# installation
# !pip install linearmodels stargazer

# packages
import math as math
import numpy as np
import pandas as pd
from scipy import stats
import statsmodels.formula.api as smf
import statsmodels.api as sm
import linearmodels as plm
from linearmodels.panel import PanelOLS

# working directory
import os as os
os.getcwd()
# os.chdir()
os.getcwd()








# We need these libraries:
# pandas = work with data tables
# PanelOLS = fixed effects regression
# statsmodels = used to add constant (intercept)

import pandas as pd
from linearmodels.panel import PanelOLS
import statsmodels.api as sm


# STEP 1: Load the dataset from the csv file
traffic = pd.read_csv("traffic.csv")

# STEP 2: Tell Python this is panel data
# Panel data = same states observed over many years
# Index must be (state, year)
traffic = traffic.set_index(["state", "year"])


# STEP 3: Choose dependent variable (what we explain)
# fatal = fatality rate (number of traffic deaths)
y = traffic["fatal"]

# STEP 4: Choose explanatory variables (what explains fatality rate)
# Example variables:
# beertax = tax on beer
# mlda = minimum legal drinking age
X = traffic[["beertax", "mlda"]]


# STEP 5: Add constant (intercept)
# Required for regression
X = sm.add_constant(X)

# STEP 6: Estimate Fixed Effects model
# entity_effects=True means STATE fixed effects
# This removes permanent differences between states
fe_model = PanelOLS(
    y,
    X,
    entity_effects=True
)


# STEP 7: Fit the model
# clustered standard errors by state (better standard errors)
fe_results = fe_model.fit(
    cov_type="clustered",
    cluster_entity=True
)


# STEP 8: Print results
print(fe_results)





# REGULAR OLS REGRESSION

# OLS does NOT need panel structure (state,year index)
# So we convert the data back to a normal table
traffic_ols = traffic.reset_index()


# What we want to explain:
# fatal = traffic fatality rate
y_ols = traffic_ols["fatal"]


# Variables that explain fatality rate:
# beertax = beer tax
# mlda = drinking age
X_ols = traffic_ols[["beertax","mlda"]]


# Add constant (intercept)
# Regression needs this
X_ols = sm.add_constant(X_ols)


# Run OLS regression
ols_model = sm.OLS(y_ols, X_ols)

# Estimate coefficients
ols_results = ols_model.fit()

# STARGAZER TABLE
# Compare OLS and Fixed Effects

from stargazer.stargazer import Stargazer

# Put both models into one table:
# 1 = OLS
# 2 = Fixed Effects
stargazer = Stargazer([ols_results, fe_results])

# Table title
stargazer.title("OLS vs Fixed Effects")

# Column names
stargazer.custom_columns(["OLS","Fixed Effects"], [1,1])

# Show table in Jupyter Notebook
display(stargazer)








# We need these libraries:
# pandas = work with data tables
# PanelOLS = fixed effects regression
# statsmodels = used to add constant (intercept)

import pandas as pd
from linearmodels.panel import PanelOLS
import statsmodels.api as sm


# STEP 1: Load the dataset from the csv file
traffic = pd.read_csv("traffic.csv")

# STEP 2: Tell Python this is panel data
# Panel data = same states observed over many years
# Index must be (state, year)
traffic = traffic.set_index(["state", "year"])


# STEP 3: Choose dependent variable (what we explain)
# fatal = fatality rate (number of traffic deaths)
y = traffic["fatal"]

# STEP 4: Choose explanatory variables (what explains fatality rate)
# Example variables:
# beertax = tax on beer
# mlda = minimum legal drinking age
X = traffic[["beertax", "mlda"]]


# STEP 5: Add constant (intercept)
# Required for regression
X = sm.add_constant(X)

# STEP 6: Estimate Fixed Effects model
# entity_effects=True means STATE fixed effects
# This removes permanent differences between states
fe_model = PanelOLS(
    
    y,   # Dependent variable (fatality rate we want to explain)
    
    X,   # Independent variables (beertax and mlda)
    
    # entity_effects=True means STATE fixed effects
    # Each state gets its own constant (intercept)
    # This removes permanent differences between states
    entity_effects=True
)

# STEP 7: Fit the model
# clustered standard errors by state (better standard errors)
fe_results = fe_model.fit(
    
    # Use clustered standard errors
    # This makes the standard errors more reliable
    # because observations from the same state may be similar
    cov_type="clustered",
    
    # Cluster by state (entity = state)
    # Allows errors to be correlated within each state over time
    cluster_entity=True
)

# STEP 8: Test if Individual (State) Effects Exist
# This test checks whether fixed effects are needed

# Null hypothesis (H0):
# No individual (state) effects → OLS is enough

# Alternative hypothesis (H1):
# Individual (state) effects exist → Fixed Effects needed

# Extract the F-test for poolability directly
print("\nF-test for individual effects:")
print(fe_results.f_pooled)





# Manual F-test for Individual Effects

import scipy.stats as stats   # needed to compute p-value


# Run pooled OLS (no fixed effects)

traffic_ols = traffic.reset_index()

y_ols = traffic_ols["fatal"]
X_ols = sm.add_constant(traffic_ols[["beertax","mlda"]])

ols_results = sm.OLS(y_ols, X_ols).fit()


# Sum of squared residuals (SSR)
SSR_pooled = sum(ols_results.resid**2)
SSR_FE = sum(fe_results.resids**2)


# Sample sizes
N = traffic.shape[0]              # total observations
G = traffic.index.levels[0].size  # number of states
K = X.shape[1]                    # number of regressors

# Compute F-statistic

F = ((SSR_pooled - SSR_FE)/(G-1)) / (SSR_FE/(N-G-K))

# Compute p-value

p_value = 1 - stats.f.cdf(F, G-1, N-G-K)


print("Manual F-statistic:", F)
print("Manual p-value:", p_value)











# ======================================
# Fixed Effects Model
# WITHOUT correcting for autocorrelation
# ======================================

import pandas as pd
from linearmodels.panel import PanelOLS
import statsmodels.api as sm


# Load data
traffic = pd.read_csv("traffic.csv")


# Set panel structure (state, year)
traffic = traffic.set_index(["state", "year"])


# Dependent variable
y = traffic["fatal"]


# Independent variables
X = traffic[["beertax", "mlda"]]


# Add constant
X = sm.add_constant(X)


# Fixed Effects model (state effects)
fe_model = PanelOLS(
    y,
    X,
    entity_effects=True
)


# Estimate model WITHOUT clustered errors
# Uses default standard errors (assumes no autocorrelation)

fe_results= fe_model.fit()
# Print results
# print(fe_results_uncorrected)

# Panel Breusch-Godfrey Test
# (similar to pbgtest() in R)
# Test for serial correlation

from scipy import stats   # Needed to calculate p-value


# STEP 1: Get residuals from Fixed Effects model
# Residuals = errors from the regression
# We test if these errors are correlated over time
residuals = fe_results.resids.reset_index()


# STEP 2: Create lagged residuals
# Lagged residual = error from previous year
# We do this separately for each state
residuals["resid_lag"] = residuals.groupby("state")["residual"].shift(1)


# STEP 3: Remove missing values
# First year in each state has no lagged value
# So we remove those rows
residuals = residuals.dropna()


# STEP 4: Regress residuals on lagged residuals
# If lagged residual explains residuals,
# then autocorrelation exists

# Dependent variable = residuals
y_bg = residuals["residual"]

# Independent variable = lagged residuals
X_bg = sm.add_constant(residuals["resid_lag"])

# Run regression
bg_model = sm.OLS(y_bg, X_bg).fit()


# STEP 5: Compute Breusch-Godfrey test statistic
# Formula:
# BG statistic = number of observations × R²

BG_stat = len(residuals) * bg_model.rsquared


# STEP 6: Compute p-value
# This tells us if autocorrelation exists

p_value = 1 - stats.chi2.cdf(BG_stat, 1)


# STEP 7: Print results

print("Panel Breusch-Godfrey test (similar to pbgtest in R)")
print("Statistic:", BG_stat)
print("p-value:", p_value)














# Breusch-Pagan Test for Heteroskedasticity
# Fixed Effects Model 

import pandas as pd
from linearmodels.panel import PanelOLS
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan

# 1) Load data
traffic = pd.read_csv("traffic.csv")

# 2) Set panel index (state, year)
traffic = traffic.set_index(["state", "year"])

# 3) Dependent variable
y = traffic["fatal"]

# 4) Regressors (same as your FE model)
X = traffic[["beertax", "mlda"]]
X = sm.add_constant(X)

# 5) Fit Fixed Effects model (uncorrected is fine for getting residuals)
fe_model = PanelOLS(y, X, entity_effects=True)
fe_results = fe_model.fit()

# 6) Get FE residuals (errors)
fe_residuals = fe_results.resids.reset_index()

# 7) Build X for BP test (must align with residuals order)
X_bp = traffic.reset_index()[["beertax", "mlda"]]
X_bp = sm.add_constant(X_bp)

# 8) Run Breusch–Pagan test
bp_test = het_breuschpagan(fe_residuals["residual"], X_bp)

print("Breusch-Pagan Test (Fixed Effects residuals)")
print("LM Statistic:", bp_test[0])
print("LM p-value:", bp_test[1])
print("F Statistic:", bp_test[2])
print("F p-value:", bp_test[3])











# Fixed Effects Models
# Corrected vs Uncorrected
# Independent version

import pandas as pd
from linearmodels.panel import PanelOLS
import statsmodels.api as sm
from stargazer.stargazer import Stargazer


# STEP 1: Load data
traffic = pd.read_csv("traffic.csv")


# STEP 2: Set panel structure
traffic = traffic.set_index(["state","year"])


# STEP 3: Dependent variable
y = traffic["fatal"]


# STEP 4: Independent variables
X = traffic[["beertax","mlda"]]


# STEP 5: Add constant
X = sm.add_constant(X)


# STEP 6: Create Fixed Effects model
fe_model = PanelOLS(
    y,
    X,
    entity_effects=True
)


# STEP 7: Uncorrected Fixed Effects model
fe_uncorrected = fe_model.fit()


# STEP 8: Clustered Fixed Effects model
fe_corrected = fe_model.fit(
    cov_type="clustered",
    cluster_entity=True
)


# STEP 9: Stargazer Table

stargazer = Stargazer([fe_uncorrected, fe_corrected])

stargazer.title("Fixed Effects: Corrected vs Uncorrected")

stargazer.custom_columns(
    ["Uncorrected FE","Clustered FE"],
    [1,1]
)

display(stargazer)














# Random Effects Model (Python version of plm random)

import pandas as pd
from linearmodels.panel import RandomEffects
import statsmodels.api as sm


# STEP 1: Load data
rice = pd.read_csv("rice.csv")


# STEP 2: Set panel structure (firm, year)
rice = rice.set_index(["firm","year"])


# STEP 3: Dependent variable
y = rice["prod"]


# STEP 4: Independent variables
X = rice[["area","labor","fert"]]


# STEP 5: Add constant
X = sm.add_constant(X)


# STEP 6: Random Effects model
random_model = RandomEffects(y, X)


# STEP 7: Estimate model
random_results = random_model.fit()


# STEP 8: Show results
print(random_results)


# OLS vs Fixed Effects vs Random Effects
# Independent Stargazer Cell

import pandas as pd
import statsmodels.api as sm
from linearmodels.panel import PanelOLS, RandomEffects
from stargazer.stargazer import Stargazer


# STEP 1: Load data
rice = pd.read_csv("rice.csv")

# OLS (Pooled OLS)

# OLS needs normal dataframe
rice_ols = rice.copy()

y_ols = rice_ols["prod"]
X_ols = rice_ols[["area","labor","fert"]]
X_ols = sm.add_constant(X_ols)

ols_model = sm.OLS(y_ols, X_ols).fit()


# Panel Structure

rice_panel = rice.set_index(["firm","year"])

y = rice_panel["prod"]
X = rice_panel[["area","labor","fert"]]
X = sm.add_constant(X)

# Fixed Effects Model

fe_model = PanelOLS(
    y,
    X,
    entity_effects=True
)

fe_results = fe_model.fit()


# =========================
# Random Effects Model
# =========================

re_model = RandomEffects(
    y,
    X
)

re_results = re_model.fit()


# =========================
# Stargazer Table
# =========================

stargazer = Stargazer([ols_model, fe_results, re_results])

stargazer.title("OLS vs Fixed Effects vs Random Effects")

stargazer.custom_columns(
    ["OLS","Fixed Effects","Random Effects"],
    [1,1,1]
)

display(stargazer)














# e) Hausman test
# Purpose: Decide whether Fixed Effects (FE) or Random Effects (RE) is better
# H0: Random Effects is correct (use RE model)
# H1: Fixed Effects is correct (use FE model)


# Import needed tools
from linearmodels.panel import PanelOLS
import numpy as np
from scipy import stats


# STEP 1: Estimate Fixed Effects model
# This model allows each firm to have its own intercept
fe_model = PanelOLS(y, X, entity_effects=True)

# Fit the model
fe_results = fe_model.fit()


# STEP 2: Random Effects model is already estimated
# It is stored in "random_results"
# So we do not need to estimate it again


# STEP 3: Get coefficients from both models
# These are beta estimates

b_FE = fe_results.params      # coefficients from Fixed Effects
b_RE = random_results.params  # coefficients from Random Effects


# STEP 4: Get covariance matrices
# Needed for Hausman formula

V_FE = fe_results.cov
V_RE = random_results.cov


# STEP 5: Difference between coefficients
# FE - RE

diff = b_FE - b_RE


# STEP 6: Calculate Hausman statistic
# Formula: (b_FE - b_RE)' * inv(V_FE - V_RE) * (b_FE - b_RE)

stat = np.dot(np.dot(diff.T, np.linalg.inv(V_FE - V_RE)), diff)


# STEP 7: Degrees of freedom
# Number of coefficients

df = len(diff)


# STEP 8: Calculate p-value from Chi-square distribution

pval = 1 - stats.chi2.cdf(stat, df)


# STEP 9: Print results

print("Hausman statistic:", stat)
print("Degrees of freedom:", df)
print("p-value:", pval)


# STEP 10: Interpretation

if pval < 0.05:
    print("Result: Reject H0 → Fixed Effects model is better")
else:
    print("Result: Do not reject H0 → Random Effects model is OK")








# f) Breusch–Pagan LM test for individual effects
# This test checks if Random Effects are needed instead of pooled OLS

# H0: No individual (firm) effects -> pooled OLS is enough
# H1: Individual effects exist -> Random Effects model is better


# STEP 1: Import libraries
import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy import stats
from linearmodels.panel import PooledOLS


# STEP 2: Load the dataset
# rice.csv must be in the same folder as the notebook
rice = pd.read_csv("rice.csv")


# STEP 3: Set panel structure
# firm = individual unit
# year = time period
rice = rice.set_index(["firm", "year"])


# STEP 4: Define dependent variable (Y)
# rice production
y = rice["prod"]


# STEP 5: Define independent variables (X)
# explanatory variables
X = rice[["area", "labor", "fert"]]


# STEP 6: Add constant (intercept)
X = sm.add_constant(X)


# STEP 7: Estimate pooled OLS model
# This is the same as POLS in R
pool = PooledOLS(y, X).fit()


# STEP 8: Get residuals from pooled OLS
# residual = actual value - predicted value
e = pool.resids


# STEP 9: Number of observations (N*T)
N_obs = int(e.shape[0])


# STEP 10: Count how many years each firm has
# T_i = number of observations for firm i
T_i = e.groupby(level=0).size().astype(float)


# STEP 11: Calculate M11
# Needed in the Breusch-Pagan formula
M11 = float(np.sum(T_i**2))


# STEP 12: Sum of squared residuals
# Σ e_it^2
CP = float(np.sum(np.asarray(e)**2))


# STEP 13: Sum residuals inside each firm
# First sum residuals over time for each firm
sum_by_i = e.groupby(level=0).sum()

# Then square and sum them
SS_i = float(np.sum(np.asarray(sum_by_i)**2))


# STEP 14: Calculate A1 statistic
# This measures how different firms are from each other
A1 = SS_i / CP - 1.0


# STEP 15: Calculate LM statistic
# Formula used in R plmtest(type="bp")
LM1 = N_obs * (1.0 / np.sqrt(2.0 * (M11 - N_obs))) * A1


# STEP 16: Convert LM into Chi-square statistic
# plmtest reports Chi-square value
chisq = float(LM1**2)


# STEP 17: Compute p-value
# Chi-square distribution with df=1
pval = 1.0 - stats.chi2.cdf(chisq, df=1)


# STEP 18: Print results
print("Breusch–Pagan LM test (same as R plmtest(type='bp'))")
print(f"chisq = {chisq:.6f}")
print("df = 1")
print(f"p-value = {pval:.8f}")


# STEP 19: Interpretation (automatic)
if pval < 0.05:
    print("\nConclusion:")
    print("Reject H0 -> individual (firm) effects are significant.")
    print("Random Effects model is better than pooled OLS.")
else:
    print("\nConclusion:")
    print("Do not reject H0 -> individual effects are not significant.")
    print("Pooled OLS may be sufficient.")





# g) Serial correlation test for Random Effects model
# Python version similar to R: pbgtest(random)
#
# ============================================================
# PURPOSE (in human words):
# We want to check if the model errors are "remembering the past".
# In other words: are residuals in year t related to residuals in year t-1, t-2, ... ?
#
# If residuals are correlated over time, standard errors and p-values can be wrong.
#
# H0 (null): No serial correlation in the errors (good)
# H1 (alt) : Serial correlation exists (bad)
# ============================================================


# -------------------------
# STEP 1: Import libraries
# -------------------------
# pandas: for data tables
# numpy: for numeric arrays
# statsmodels: for OLS regression
# scipy.stats: for chi-square distribution (to get p-values)
# linearmodels.panel: for Random Effects estimation
import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy import stats
from linearmodels.panel import RandomEffects


# -------------------------
# STEP 2: Load data
# rice.csv must be in the same folder
# -------------------------
# If this fails, your file is not in the working directory or the name is wrong.
rice = pd.read_csv("rice.csv")


# -------------------------
# STEP 3: Set panel structure
# firm = individual unit (like person/company)
# year = time period
# -------------------------
# Panel data = same firms observed over multiple years.
# Setting a MultiIndex makes group operations (like firm means) easy.
rice = rice.set_index(["firm", "year"])


# -------------------------
# STEP 4: Define variables
# Dependent variable (DV) = production
# Independent variables (IVs) = area, labor, fertilizer
# -------------------------
y = rice["prod"]                           # DV

X = rice[["area", "labor", "fert"]]        # IVs

# Add constant (intercept) to X
# Without this, your regression has no intercept term.
X = sm.add_constant(X)


# -------------------------
# STEP 5: Estimate Random Effects model
# Same as plm(..., model="random") in R
# -------------------------
# Random Effects assumes:
# - each firm has its own intercept component (random)
# - that firm effect is not correlated with X
re_res = RandomEffects(y, X).fit()


# -------------------------
# STEP 6: Get theta parameter
# Theta controls how strong the RE "demeaning" is.
#
# For RE, we use a partial demeaning:
# y* = y - theta * firm_mean(y)
# X* = X - theta * firm_mean(X)
#
# If theta = 0 -> no demeaning (like pooled OLS)
# If theta = 1 -> full demeaning (like Fixed Effects / within estimator)
#
# linearmodels stores theta as an array, so we extract a single number.
# -------------------------
theta_raw = np.asarray(re_res.theta)
theta_val = float(theta_raw.reshape(-1)[0])


# -------------------------
# STEP 7: Random Effects transformation (manual)
#
# Why do we do this?
# Because the panel BG test in R is applied to residuals from the RE-transformed model.
#
# firm_mean(y) means: for each firm, average y across all years.
# Then subtract theta * that firm mean from each observation.
# -------------------------
y_bar = y.groupby(level=0).transform("mean")   # firm means of y
X_bar = X.groupby(level=0).transform("mean")   # firm means of each X column

# Transformed (star) variables
y_star = y - theta_val * y_bar
X_star = X - theta_val * X_bar


# -------------------------
# STEP 8: Compute residuals from transformed model
#
# We run OLS on transformed data:
# y_star = X_star * beta + error_star
#
# Then error_star are the residuals we test for serial correlation.
# -------------------------
ols_star = sm.OLS(np.asarray(y_star), np.asarray(X_star)).fit()

# Put residuals back into a pandas Series with the original panel index.
# This helps when we create lags.
e_star = pd.Series(
    ols_star.resid,
    index=y.index,
    name="e_star"
)


# -------------------------
# STEP 9: Choose lag order
#
# BG test needs a number of lags (how many past years of errors to include).
# Here you set it equal to T (number of years per firm).
#
# Example:
# if each firm has 8 years, then order = 8.
# -------------------------
T = int(y.groupby(level=0).size().iloc[0])  # number of time periods per firm (assumes balanced panel)
order = T                                   # lag order used in the test


# -------------------------
# STEP 10: Build BG auxiliary regression dataset
#
# The BG idea:
# Regress current residual e_t on:
#  1) the regressors X* (controls)
#  2) lagged residuals e_{t-1}, e_{t-2}, ..., e_{t-order}
#
# If lagged residuals are significant as a group -> residuals are serially correlated.
# -------------------------
aux = pd.DataFrame(index=y.index)
aux["e"] = e_star

# Create lagged residuals.
# IMPORTANT NOTE (matching your comment):
# This uses a plain shift() on the stacked panel.
# That mimics the "stacked" lag structure used by some implementations.
# (If you wanted lags *within each firm only*, you'd use groupby(level=0).shift(k).)
for k in range(1, order + 1):
    aux[f"e_lag{k}"] = e_star.shift(k)

# Add transformed regressors to the auxiliary dataset
for col in X_star.columns:
    aux[col] = X_star[col]

# Drop rows where lags are missing (first "order" rows become NaN after shifting)
aux = aux.dropna()


# -------------------------
# STEP 11: Run the auxiliary regression
# -------------------------
# Dependent variable: current residual
dep = aux["e"].to_numpy()

# RHS: transformed regressors + lagged residuals
rhs_cols = list(X_star.columns) + [f"e_lag{k}" for k in range(1, order + 1)]
rhs = aux[rhs_cols].to_numpy()

# Run OLS (no constant added here because X_star already includes const column)
aux_fit = sm.OLS(dep, rhs).fit()


# -------------------------
# STEP 12: Compute LM statistic and p-value
#
# LM = n * R^2
#
# Under H0 (no serial correlation), LM ~ Chi-square(df = number of lags = order)
# Big LM => small p-value => evidence of serial correlation.
# -------------------------
LM = float(aux.shape[0] * aux_fit.rsquared)
df = order
pval = 1 - stats.chi2.cdf(LM, df=df)


# -------------------------
# STEP 13: Print results (similar style to R)
# -------------------------
print("Breusch-Godfrey/Wooldridge test for serial correlation in panel models")
print("data: prod ~ area + labor + fert")
print(f"chisq = {LM:.3f}, df = {df}, p-value = {pval:.8f}")
print("alternative hypothesis: serial correlation in idiosyncratic errors")


# -------------------------
# STEP 14: Interpretation (the only part most students care about)
# -------------------------
# Rule of thumb:
# p-value < 0.05  -> reject H0 -> serial correlation exists
# p-value >= 0.05 -> fail to reject H0 -> no evidence of serial correlation
if pval < 0.05:
    print("\nConclusion:")
    print("Reject H0 -> Serial correlation exists (errors are correlated over time).")
else:
    print("\nConclusion:")
    print("Do not reject H0 -> No evidence of serial correlation.")





# h) Studentized Breusch-Pagan test (same as R bptest(..., studentize=TRUE))

import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan

# -------------------------
# STEP 1: Load data
# -------------------------
traffic = pd.read_csv("traffic.csv")

# -------------------------
# STEP 2: Define variables
# -------------------------
y = traffic["fatal"]

X = traffic[["beertax","spircons","unrate","perincK"]]

# Add constant
X = sm.add_constant(X)

# -------------------------
# STEP 3: Run OLS model
# (bptest uses OLS residuals)
# -------------------------
ols_model = sm.OLS(y,X).fit()

# -------------------------
# STEP 4: Studentized Breusch-Pagan test
# Same as:
# bptest(..., studentize=TRUE) in R
# -------------------------
bp_test = het_breuschpagan(
    ols_model.resid,
    X
)

# Results
LM_stat = bp_test[0]
LM_pval = bp_test[1]
F_stat = bp_test[2]
F_pval = bp_test[3]

print("Studentized Breusch-Pagan test")

print("LM statistic =", LM_stat)
print("df =", X.shape[1]-1)
print("p-value =", LM_pval)

if LM_pval < 0.05:
    print("\nConclusion: Reject H0 -> heteroskedasticity exists.")
else:
    print("\nConclusion: Do not reject H0 -> no heteroskedasticity.")





# Python version of:

# random <- plm(score~judgexp+ppepisodexp,...)
# fixed <- plm(score~judgexp+ppepisodexp,...)
# phtest(fixed,random)
# stargazer(fixed,random)

# -------------------------
# STEP 1: Import libraries
# -------------------------

import pandas as pd
import statsmodels.api as sm
from linearmodels.panel import RandomEffects, PanelOLS
import numpy as np
from scipy import stats
from stargazer.stargazer import Stargazer


# -------------------------
# STEP 2: Load data
# -------------------------

dancing = pd.read_csv("dancingwiththestars.csv")


# -------------------------
# STEP 3: Panel index
# Same as:
# index=c("team","time")
# -------------------------

dancing = dancing.set_index(["team","time"])


# -------------------------
# STEP 4: Variables
# Same as:
# score ~ judgexp + ppepisodexp
# -------------------------

y = dancing["score"]

X = dancing[["judgexp","ppepisodexp"]]

X = sm.add_constant(X)


# -------------------------
# STEP 5: Random Effects
# Same as:
# model="random"
# -------------------------

random_model = RandomEffects(y,X)

random_results = random_model.fit()


# -------------------------
# STEP 6: Fixed Effects
# Same as:
# model="within"
# -------------------------

fixed_model = PanelOLS(
    y,
    X,
    entity_effects=True
)

fixed_results = fixed_model.fit()


# -------------------------
# STEP 7: Hausman Test
# Same as:
# phtest(fixed,random)
# -------------------------

b_FE = fixed_results.params
b_RE = random_results.params

V_FE = fixed_results.cov
V_RE = random_results.cov

diff = b_FE - b_RE

stat = diff.T @ np.linalg.inv(V_FE - V_RE) @ diff

df = len(diff)

pval = 1 - stats.chi2.cdf(stat,df)

print("Hausman Test")
print("Statistic =", stat)
print("p-value =", pval)

if pval < 0.05:
    print("Use Fixed Effects")
else:
    print("Use Random Effects")


# -------------------------
# STEP 8: Stargazer table
# Same as:
# stargazer(fixed,random)
# -------------------------

stargazer = Stargazer([
fixed_results,
random_results
])

# Title of table
stargazer.title("Results")

# Labels for models (important!)
stargazer.custom_columns(
["Fixed Effects","Random Effects"],
[1,1]
)

stargazer





# Exercise 4 - Grunfeld dataset

import pandas as pd
import statsmodels.api as sm
import numpy as np
from linearmodels.panel import PooledOLS, PanelOLS, RandomEffects
from scipy import stats


# Load data
df = pd.read_csv("grunfeld.csv")

# Remove useless column
df = df.drop(columns=["Unnamed: 0"])


# Set panel index (firm i and time t)
df = df.set_index(["firm","year"])


# Dependent variable
y = df["inv"]


# Independent variables
X = df[["value","capital"]]

X = sm.add_constant(X)


print("Data ready")


# Estimate models

pols = PooledOLS(y,X).fit()

fe = PanelOLS(
    y,
    X,
    entity_effects=True
).fit()

re = RandomEffects(y,X).fit()


print("Pooled OLS")
print(pols)

print("\nFixed Effects")
print(fe)

print("\nRandom Effects")
print(re)


# Hausman test

b_FE = fe.params
b_RE = re.params

V_FE = fe.cov
V_RE = re.cov

diff = b_FE - b_RE

H = diff.T @ np.linalg.inv(V_FE - V_RE) @ diff

df_h = len(diff)

pval = 1 - stats.chi2.cdf(H,df_h)

print("Hausman Test")
print("Statistic =", H)
print("p-value =", pval)

if pval < 0.05:
    print("Conclusion: Use Fixed Effects")
else:
    print("Conclusion: Use Random Effects")


from stargazer.stargazer import Stargazer

stargazer = Stargazer([pols,fe,re])

stargazer.title("Grunfeld Investment Model")

stargazer.dependent_variable_name("Investment")

stargazer.custom_columns(
["Pooled OLS","Fixed Effects","Random Effects"],
[1,1,1]
)

stargazer



