---
title: "Monte Carlo simulations"
output: 
  flexdashboard::flex_dashboard:
    vertical_layout: fill
runtime: shiny
---
```{r setup, include=FALSE}

################ START #####DO IT ONCE############
###### Check, installation and loading of required packages #######
requiredPackages = c( "readr", "wesanderson", "flexdashboard","shiny", "copula", "mc2d", "EnvStats","randomForest", "rpart", "shiny", "dplyr" ) # list of required packages
for(i in requiredPackages){if(!require(i,character.only = TRUE)) install.packages(i)}
for(i in requiredPackages){if(!require(i,character.only = TRUE)) library(i,character.only = TRUE) } 


library("flexdashboard")
library("shiny")
library("copula")
library("mc2d")
library("EnvStats")
library("randomForest")
library("rpart")
library("shiny")
library("dplyr")


################END##########################

pal_col <- wes_palette(n = 10,type = "continuous", name="Royal1")



```


1. Monte Carlo Intro
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs: 



```{r echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Input for number of Monte Carlo samples
sliderInput("N", "Number of Monte Carlo simulations", min = 1000, max = 20000, value = 10000, step = 1000)

```

Column 
-----------------------------------------------------------------------

### MC

<h3>Monte Carlo Simulation: Estimation of Circle Area</h3>

<h3>Overview</h3>
<p>
The Monte Carlo method is a computational algorithm that uses random sampling to solve problems that might be deterministic in nature. In this example, we use Monte Carlo simulation to estimate the value of <i>π</i> by calculating the area of a quarter-circle inscribed in a square.
</p>

<h3>How It Works</h3>
<ol>
  <li>Random points (<i>x</i>, <i>y</i>) are uniformly generated within a unit square where <i>0 ≤ x ≤ 1</i> and <i>0 ≤ y ≤ 1</i>.</li>
  <li>The distance of each point from the origin (0, 0) is calculated using the formula:
    <br>
    <code>distance = sqrt(x² + y²)</code>
  </li>
  <li>If the distance is less than or equal to 1, the point lies inside the quarter-circle. Otherwise, it lies outside.</li>
  <li>The proportion of points inside the quarter-circle to the total number of points in the square approximates the ratio of the quarter-circle area to the square area.</li>
  <li>The value of <i>π</i> is then estimated as:
    <br>
    <code>π ≈ 4 * (number of points inside the circle / total number of points)</code>
  </li>
</ol>

<h3>Steps in the Simulation</h3>
<ol>
  <li><strong>Generate random points:</strong> Randomly generate <i>N</i> points in the unit square.</li>
  <li><strong>Check point location:</strong> Calculate the distance of each point from the origin and determine if it lies inside the quarter-circle.</li>
  <li><strong>Calculate the proportion:</strong> Count the number of points inside the circle and divide it by the total number of points.</li>
  <li><strong>Estimate <i>π</i>:</strong> Multiply the proportion by 4 to approximate the value of <i>π</i>.</li>
</ol>

<h3>Insights</h3>
<ul>
  <li>The accuracy of the Monte Carlo estimation increases as the number of simulations (<i>N</i>) grows.</li>
  <li>The randomness of point generation ensures that the method is unbiased but introduces variability in the results, especially for smaller <i>N</i>.</li>
  <li>This method can also be generalized to calculate the area under more complex curves or shapes using random sampling techniques.</li>
</ul>

<h3>Practical Applications</h3>
<ul>
  <li>Estimating areas or volumes of irregular shapes.</li>
  <li>Solving integrals that cannot be evaluated analytically.</li>
  <li>Financial modeling, risk analysis, and other stochastic processes where randomness plays a critical role.</li>
</ul>

Column 
-----------------------------------------------------------------------

### Model

```{r}
renderPlot({
  # Number of points
  N <- input$N
  
  # Generate random points (x, y)
  x <- runif(N, 0, 1)  # Random x-coordinates in [0,1]
  y <- runif(N, 0, 1)  # Random y-coordinates in [0,1]
  
  # Distance from origin (0,0)
  dist <- sqrt(x^2 + y^2)
  
  # Points inside the quarter-circle (distance <= 1)
  inside <- ifelse(dist <= 1, 1, 0)
  
  # Calculate the proportion of points inside the circle
  prob_inside <- sum(inside) / N
  
  # Approximation of Pi
  pi_approx <- prob_inside * 4
  
  # Plot the points
  plot(x, y, col = ifelse(inside == 1, "red", "gray"), pch = 16, cex = 0.5,
       main = paste("Monte Carlo Simulation: N =", N, "\nPi Approximation =", round(pi_approx, 6)),
       xlab = "x", ylab = "y")
})

```

2. Monte Carlo Optimization
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs:

```{r echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Inputs for Monte Carlo Optimization
sliderInput("N_opt", "Number of Monte Carlo simulations", min = 1000, max = 20000, value = 10000, step = 1000)
sliderInput("a", "Range Start (a)", min = 1, max = 2, value = 1, step = 0.01)
sliderInput("b", "Range End (b)", min = 1, max = 2, value = 2, step = 0.01)
```

Column 
-----------------------------------------------------------------------

### MC optym.

<h3>Monte Carlo Optimization of a Function</h3> <h3>Overview</h3> <p> Monte Carlo optimization is a technique that uses random sampling to find the global optimum (maximum or minimum) of a function within a defined range. Unlike traditional optimization methods that may converge to local optima, Monte Carlo simulation can explore the entire solution space to identify the global optimum. </p> <h3>Function</h3> <p> The function to optimize is: <br> <code>f(x) = sin(18x * log(x)) / x</code> <br> This function has multiple local minima and maxima in the interval <code>[a, b]</code>, making it an ideal candidate for Monte Carlo optimization. </p> <h3>Steps in the Optimization</h3> <ol> <li>Generate <i>N</i> random points <i>x</i> uniformly in the range <code>[a, b]</code>.</li> <li>Calculate the function value <code>f(x)</code> for each point.</li> <li>Identify the point with the minimum or maximum function value.</li> </ol> <h3>Monte Carlo Optimization Result</h3>

<h3>Insights</h3> <ul> <li>Monte Carlo optimization is particularly useful for functions with multiple local optima.</li> <li>By sampling uniformly across the range, this method avoids being trapped in local minima or maxima.</li> <li>The accuracy of the solution improves with an increasing number of simulations (<i>N</i>).</li> </ul> <h3>Comparison with Other Methods</h3> <ul> <li>R's <code>optimize</code> function uses a deterministic approach and may converge to a local optimum.</li> <li>The Monte Carlo method explores the entire solution space and is robust against local optima.</li> </ul>


Column 
-----------------------------------------------------------------------


### graph 
```{r}
renderPlot({
  # Parameters
  N <- input$N_opt
  a <- input$a
  b <- input$b
  
  # Generate random points
  x <- runif(N, a, b)
  
  # Function to optimize
  f <- function(x) sin(18 * x * log(x)) / x
  
  # Calculate function values
  y <- f(x)
  
  # Find minimum
  min_index <- which.min(y)
  x_min <- x[min_index]
  y_min <- y[min_index]
  
  # Plot the function
  curve(f, from = a, to = b, col = "black", lwd = 2,
        main = paste("Monte Carlo Optimization: N =", N),
        xlab = "x", ylab = "f(x)")
  points(x, y, col = "gray", pch = 16, cex = 0.5) # Random points
  points(x_min, y_min, col = "red", pch = 19, cex = 1.5) # Global minimum
  abline(v = x_min, col = "red", lwd = 2, lty = 2)
  text(x_min, y_min, labels = paste0("Min: x=", round(x_min, 4), ", f(x)=", round(y_min, 4)),
       pos = 4, col = "red")
})
```



3. Profitability Analysis
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs:

```{r echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Inputs for Monte Carlo simulation
sliderInput("cost_param", "Cost per Product (C) [PLN]", min = 50, max = 120, value = 90, step = 10)
sliderInput("price_min", "Minimum Price (PLN)", min = 70, max = 90, value = 90, step = 10)
sliderInput("price_max", "Maximum Price (PLN)", min = 90, max = 150, value = 120, step = 10)
sliderInput("sales_min", "Minimum Units Sold",  min = 90, max = 200, value = 100, step = 10)
sliderInput("sales_peak", "Most Likely Units Sold", min = 200, max = 700, value = 350, step = 10)
sliderInput("sales_max", "Maximum Units Sold", min = 500, max = 5000, value = 1000, step = 10)
sliderInput("damaged_rate", "Damaged Packages (%)", min = 0, max = 20, value = 9, step = 1)
sliderInput("iterations", "Number of Simulations", min = 10000, max = 30000, value = 20000, step = 1000)
sliderInput("var_level", "VaR Confidence Level (%)", min = 1, max = 10, value = 5, step = 1)  
  
```

Column 
-----------------------------------------------------------------------
### MC VaR

<h2>Profitability Simulation</h2>

<h3>Overview</h3>
<p>
This simulation estimates the monthly profitability of a delivery company using Monte Carlo methods. It incorporates randomness in sales, prices, and damaged packages to model the uncertainty in real-world scenarios. The simulation outputs a distribution of potential profits, helping to assess the financial viability of the business.
</p>

<h3>Key Variables</h3>
<ul>
  <li><strong>Cost per Product (C):</strong> The production cost for a single unit.</li>
  <li><strong>Sales:</strong> The number of units sold, modeled using a triangular distribution with parameters for the minimum, maximum, and most likely sales.</li>
  <li><strong>Prices:</strong> The selling price of a product, modeled using a uniform distribution within a defined range.</li>
  <li><strong>Damaged Packages:</strong> A percentage of sold items that are damaged during delivery, incurring additional costs.</li>
</ul>

<h3>Simulation Workflow</h3>
<ol>
  <li>Generate random values for sales using a triangular distribution.</li>
  <li>Generate random selling prices using a uniform distribution.</li>
  <li>Calculate the number of damaged packages using a binomial distribution.</li>
  <li>Compute the total revenue, total costs, and profit:
    <ul>
      <li><strong>Revenue:</strong> Based on sales and prices, excluding damaged packages.</li>
      <li><strong>Total Costs:</strong> Sum of production costs and additional costs for damaged items.</li>
      <li><strong>Profit:</strong> Revenue minus total costs.</li>
    </ul>
  </li>
  <li>Visualize the distribution of profits and calculate key metrics like the mean and Value at Risk (VaR).</li>
</ol>

<h2>What is Value at Risk (VaR)?</h2>

<h3>Definition</h3>
<p>
Value at Risk (VaR) is a statistical measure used to estimate the potential loss in value of a portfolio or business operation over a specific time frame, under normal market conditions, at a given confidence level. For example, a 95% VaR of -10,000 PLN means there is a 5% probability of experiencing a loss greater than 10,000 PLN.
</p>

<h3>How VaR is Used in Monte Carlo Simulations</h3>
<p>
Monte Carlo methods provide a powerful tool for calculating VaR by simulating a wide range of potential outcomes. Here's how it works:
</p>
<ol>
  <li>Generate random scenarios for uncertain variables (e.g., sales, prices, damages).</li>
  <li>Compute the profit for each scenario.</li>
  <li>Sort the simulated profits and identify the quantile corresponding to the desired confidence level.</li>
</ol>

<h3>Why VaR Matters</h3>
<ul>
  <li>Helps businesses and investors understand the worst-case financial outcomes under normal conditions.</li>
  <li>Provides a quantifiable risk measure to support decision-making.</li>
  <li>Allows for comparison of risk across different scenarios or strategies.</li>
</ul>

<h2>Experiment and Stress Testing</h2>

<h3>Interactive Exploration</h3>
<p>
Students are encouraged to experiment with the simulation by modifying key variables such as:
</p>
<ul>
  <li><strong>Production Cost (C):</strong> Observe how changes in production cost affect profitability.</li>
  <li><strong>Price Range:</strong> Assess the impact of different pricing strategies.</li>
  <li><strong>Sales Distribution:</strong> Explore how shifts in expected sales volumes influence results.</li>
  <li><strong>Damaged Packages:</strong> Test the effect of increasing or decreasing the rate of damaged parcels.</li>
</ul>

<h3>Stress Testing</h3>
<p>
Perform stress tests to identify which variables have the most significant impact on profitability and risk:
</p>
<ol>
  <li>Increase or decrease the <strong>production cost</strong> and observe its effect on the mean profit and VaR.</li>
  <li>Widen or narrow the <strong>price range</strong> and examine changes in the profit distribution.</li>
  <li>Adjust the parameters of the <strong>sales distribution</strong> (minimum, peak, maximum) to see how variability in demand affects outcomes.</li>
  <li>Simulate extreme scenarios, such as a sharp increase in <strong>damaged packages</strong>, to assess resilience under adverse conditions.</li>
</ol>

<h2>Insights from the Simulation</h2>
<ul>
  <li>The mean profit provides an average expectation of monthly profitability.</li>
  <li>The VaR metric identifies the worst-case profit threshold at a given confidence level (e.g., 95%).</li>
  <li>Stress testing helps pinpoint the most sensitive variables, guiding strategic decision-making to mitigate risks.</li>
</ul>


Column 
-----------------------------------------------------------------------



### MC VaR symulation

```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}

  renderPlot({
  # Parameters from inputs
  cost <- 90
  price_min <- 90
  price_max <- 120
  sales_min <- 100
  sales_peak <- 350
  sales_max <- 1000
  damaged_rate <- 0.9  # Convert percentage to proportion
  iterations <- 2000
  var_level <- 0.05
    # Parameters from inputs
  cost       <- input$cost_param
  price_min  <- input$price_min
  price_max  <- input$price_max
  sales_min  <- input$sales_min
  sales_peak <- input$sales_peak
  sales_max  <- input$sales_max
  damaged_rate <- input$damaged_rate / 100  # Convert percentage to proportion
  iterations   <- input$iterations
  var_level    <- input$var_level / 100  # Convert percentage to proportion
  
  
  # Simulating data
  sales <- rtri(iterations, min = sales_min,  max = sales_max, mode = sales_peak) # Triangular distribution
  prices <- runif(iterations, min = price_min, max = price_max)                     # Uniform distribution

  
  # Calculating revenue, costs, and profit
  revenue <- (sales - damaged_rate *sales) * prices
  total_cost <- sales * cost +  damaged_rate *sales*(cost + 10)  # Assume extra 10 PLN cost per damaged package
  profit <- revenue - total_cost
  
  # Calculate VaR
  VaR <- quantile(profit, probs = var_level)
  
  # Plotting profit distribution
  hist(profit, breaks = 50, col = "lightblue", border = "white",
       main = "Profit Distribution with VaR", xlab = "Profit (PLN)", ylab = "Frequency")
  abline(v = mean(profit), col = "red", lwd = 2, lty = 2)
  abline(v = VaR, col = "blue", lwd = 2, lty = 2)
  text(mean(profit), max(table(cut(profit, 50))) * 0.6, 
       paste("Mean Profit =", round(mean(profit), 0)), col = "red")
  text(VaR, max(table(cut(profit, 50))) * 0.7, 
       paste("VaR (", var_level, "%) =", round(VaR, 0)), col = "blue")
})


```



4. Copula Functions
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs: 

```{r echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Slider inputs to modify copula parameters
sliderInput("N", "Number of Observations (N)", min = 1000, max = 10000, value = 5000, step = 1000)
sliderInput("tau", "Copula Dependency Parameter (τ)", min = 0.1, max = 5, value = 2, step = 0.1)
sliderInput("alpha1", "Alpha for r1 (Beta Distribution)", min = 1, max = 10, value = 5, step = 0.1)
sliderInput("beta1", "Beta for r1 (Beta Distribution)", min = 1, max = 10, value = 2, step = 0.1)
sliderInput("alpha2", "Alpha for r2 (Beta Distribution)", min = 1, max = 10, value = 2, step = 0.1)
sliderInput("beta2", "Beta for r2 (Beta Distribution)", min = 1, max = 10, value = 2, step = 0.1)
```

Column 
-----------------------------------------------------------------------

### Model

<h3>Introduction to Copula Functions</h3> <p> Copula functions are used to model dependencies between random variables, regardless of their marginal distributions. Unlike traditional methods, copulas allow flexible modeling of joint distributions by separating the dependency structure from the marginal distributions. This makes them particularly useful in finance, insurance, and risk management, where non-linear dependencies often occur. </p> <h3>What is a Copula?</h3> <p> A copula is a function that joins or "couples" multivariate distribution functions to their one-dimensional marginal distribution functions. According to Sklar's Theorem: <br> <code>H(X, Y) = C(F(X), G(Y))</code> <br> where <code>H</code> is the joint cumulative distribution function, <code>F(X)</code> and <code>G(Y)</code> are the marginal cumulative distribution functions, and <code>C</code> is the copula that describes the dependency structure. </p> <h3>Example: Frank Copula with Beta Marginal Distributions</h3>

<h3>Insights</h3> <ul> <li>The Frank copula is suitable for modeling dependencies with moderate strength.</li> <li>The beta distributions for <code>r1</code> and <code>r2</code> allow flexibility in shaping the marginals.</li> <li>The dependency structure is controlled independently using the parameter <code>τ</code>, making the copula approach highly adaptable.</li> </ul> <h3>Applications</h3> <ul> <li><strong>Finance:</strong> Modeling dependencies between asset returns or risk factors.</li> <li><strong>Insurance:</strong> Understanding joint probabilities of events, such as simultaneous claims.</li> <li><strong>Statistics:</strong> Simulating dependent data with arbitrary marginals for research and teaching.</li> </ul> 

<h3>Interactive Copula Simulations</h3>

<h4>Introduction</h4>
<p>
Below you will find multiple ready-to-use R code snippets that demonstrate how to simulate and visualize different copula functions with various marginal distributions. You can copy and run these sections in their R environment to explore the behavior of different copula models and analyze dependencies between variables. Each example is self-contained and includes visualizations of the resulting distributions.
</p>

<h3>Examples of Copula Functions</h3>

<h4>How to Use:</h4>
<ol>
  <li>Copy the desired section of code.</li>
  <li>Paste it into your R environment.</li>
  <li>Run the code to see the results and visualize the dependencies.</li>
</ol>



```{r, eval=F, echo=T}

###################################################################
# Examples --> run section by section
###################################################################

############# Section 1a ######################################
############ Normal copula with normal and normal marginal distributions
N <- 2000 
rho<- 0.7 # correlation coeficient 
copula.r1.r2 <- mvdc(normalCopula(rho, dim = 2), 
                     margins=c("norm","norm"),
                     paramMargins=list(list(0.2,0.2),list(0.2,0.2))) 
r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))

# Graph 
par(mfrow=c(2,2))
par(mar=c(2,2,5,0))
hist <- hist(r1.r2[,1], xlab="r1", ylab="", main="Distribution of r1" )
par(mar=c(1,0,2,0))
persp(copula.r1.r2, dMvdc, xlim = c(0, 0.4), ylim = c(0, 0.4), ticktype = "simple", xlab="r1", ylab="r1", main="PDF" )
par(mar=c(2,2,5,0))
plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
hist <- hist(r1.r2[,2], xlab="r2", ylab="", main="Distribution of r2" )

############# Section 1b ################################
############ Normal copula with uniform and uniform marginal distributions
N <- 2000 
rho<- 0.3 # correlation coefficient 
copula.r1.r2 <- mvdc(normalCopula(rho, dim = 2), 
                     margins=c("unif","unif"),
                     paramMargins=list(list(0,1),list(0,1))) 
r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))
# Graph 
par(mfrow=c(2,2))
par(mar=c(2,2,5,0))
hist <- hist(r1.r2[,1], xlab="r1", ylab="", main="Distribution of r1" )
par(mar=c(0,0,1,0))
persp(copula.r1.r2, dMvdc, xlim = c(-0.1, 1.1), ylim = c(-0.1, 1.1), ticktype = "simple", xlab="r1", ylab="r1",  main="PDF" )
par(mar=c(2,2,5,0))
plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
hist <- hist(r1.r2[,2], xlab="r2", ylab="", main="Distribution of r2" )
title("Normal copula", outer=TRUE) 
############# END Section 1 ###############################

############# Section 2 ####################################
############# Frank copula with beta marginal distributions 
N <- 5000 
tau <- 2 
copula.r1.r2<-mvdc(frankCopula(tau),
                   margins=c("beta", "beta"),
                   paramMargins=list(list(5,2), list(2,2)))
r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))
# Graph 
par(mfrow=c(2,2))
par(mar=c(2,2,5,0))
hist <- hist(r1.r2[,1], xlab="r1", ylab="", main="Distribution of r1" )
par(mar=c(0,0,1,0))
persp(copula.r1.r2, dMvdc, xlim = c(0, 1), ylim = c(0, 1), ticktype = "simple", xlab="r1", ylab="r1",  main="PDF")
par(mar=c(2,2,5,0))
plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
hist <- hist(r1.r2[,2], xlab="r2", ylab="", main="Distribution of r2" )
title("Frank Copula", outer=TRUE) 
############# End Section 2 ##################################

############# Section 3a ##############################################
############# Gumbel copula (tau = 4 ) with marginal with marginal uniform distributions 
N <- 5000 
tau <- 4  
copula.r1.r2 <- mvdc(gumbelCopula(tau), 
                     margins=c("unif", "unif"),
                     paramMargins=list(list(0,1), list(0,1)))
r1.r2 <- rMvdc(N, copula.r1.r2)
# Graph 
par(mfrow=c(2,2), oma=c(0,0,2,0), mar=c(2,2,5,0))
hist <- hist(r1.r2[,1], xlab="r1", ylab="", main="Distribution of r1" )
par(mar=c(0,0,1,0))
persp(copula.r1.r2, dMvdc, xlim = c(0, 1), ylim = c(0, 1), ticktype = "simple", xlab="r1", ylab="r1",  main="PDF" )
par(mar=c(2,2,5,0))
plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
hist <- hist(r1.r2[,2], xlab="r2", ylab="", main="Distribution of r2" )
title("Gumbel Cipula", outer=TRUE) 

############# Section 3b ##############################################
############# Rotated 90' Gumbel copula (tau = - 4 ) with marginal uniform distributions 
N <- 5000 
tau <- - 4  
copula.r1.r2 <- mvdc(r90GumbelCopula(tau), 
                     margins=c("unif", "unif"),
                     paramMargins=list(list(0,1), list(0,1)))
r1.r2 <- rMvdc(N, copula.r1.r2)
# Graph 
par(mfrow=c(2,2), oma=c(0,0,2,0), mar=c(2,2,5,0))
hist <- hist(r1.r2[,1], xlab="r1", ylab="", main="Distribution of r1" )
par(mar=c(0,0,1,0))
persp(copula.r1.r2, dMvdc, xlim = c(0, 1), ylim = c(0, 1), ticktype = "simple", xlab="r1", ylab="r1",  main="PDF" )
par(mar=c(2,2,5,0))
plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
hist <- hist(r1.r2[,2], xlab="r2", ylab="", main="Distribution of r2" )
title("r90Gumbel", outer=TRUE) 

############# Section 4a ##############################################
############# Clayton copula (tau = 5 ) uniform marginal distributions
N <- 5000 
tau <- 4  
copula.r1.r2 <- mvdc(claytonCopula(5), 
                     margins=c("unif", "unif"),
                     paramMargins=list(list(0,1), list(0,1)))
r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))
# Graph 
par(mfrow=c(2,2))
par(mar=c(2,2,5,0))
hist <- hist(r1.r2[,1], xlab="r1", ylab="", main="Distribution of r1" )
par(mar=c(0,0,1,0))
persp(copula.r1.r2, dMvdc, xlim = c(0, 1), ylim = c(0, 1), ticktype = "simple", xlab="r1", ylab="r1",  main="PDF" )
par(mar=c(2,2,5,0))
plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
hist <- hist(r1.r2[,2], xlab="r2", ylab="", main="Distribution of r2" )
title(" Clayton Copula ", outer=TRUE) 
############# Section 3b ##############################################
############# Rotated 90' Clayton  copula (tau = - 5 ) with marginal uniform distributions 
N <- 5000 
tau <- 4  
copula.r1.r2 <- mvdc(r90ClaytonCopula(-5), 
                     margins=c("unif", "unif"),
                     paramMargins=list(list(0,1), list(0,1)))
r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))
# Graph 
par(mfrow=c(2,2))
par(mar=c(2,2,5,0))
hist <- hist(r1.r2[,1], xlab="r1", ylab="", main="Distribution of r1" )
par(mar=c(0,0,1,0))
persp(copula.r1.r2, dMvdc, xlim = c(0, 1), ylim = c(0, 1), ticktype = "simple", xlab="r1", ylab="r1",  main="PDF" )
par(mar=c(2,2,5,0))
plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
hist <- hist(r1.r2[,2], xlab="r2", ylab="", main="Distribution of r2" )
title("Rotated Clayton Copula ", outer=TRUE)

###  Animation - comparison of beta distributions #################

################################################################### 
# Change the parameters in the selected section and run it
rho <-  - 0.9  # You can change the correlation coefficient rho 
###################################################################
y.factor11 <- c( seq(0.6,1.4, by = 0.2), rep(1.4, 4), seq(1.4,0.6 , by = -0.2), rep(0.6, 8) ) 
y.factor12 <- c( seq(0.6,1.4, by = 0.2), rep(1.4, 4), rep(1.4, 4), rep(1.4,4), seq(1.4,0.6 , by = -0.2)  )  
y.factor21 <- c( rep(0.6, 5), seq(0.7,1.4, by = 0.2),rep(1.4, 4), seq(1.4,0.6 , by = -0.2), rep(0.6,4) )
y.factor22 <- c( rep(0.6, 5), seq(0.7,1.4, by = 0.2), seq(1.4,0.6 , by = -0.2),rep(0.6,8))
y<- cbind(y.factor11,y.factor12,y.factor21,y.factor22)
iter.y <- length(y.factor11)
x <- seq(0.001, 0.999, length=2000)
N <- length(x)
# Loop of animation
saveHTML({
  ani.options(interval = 0.5, nmax = 30)
  for (j in 1:iter.y) {
    
    copula.r1.r2 <- mvdc(normalCopula(rho, dim = 2), 
                         margins=c("beta", "beta"),
                         paramMargins=list(list(y.factor11[j],y.factor12[j]), list(y.factor21[j],y.factor22[j])))
    r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))
    
    # Graph 
    par(mfrow=c(2,2), oma=c(0,0,2,0))
    par(mar=c(2,2,5,0))
    hist <- hist(r1.r2[,1], xlab="r1", ylab="", ylim = c(0,N/3) , main=paste0("r1 beta[",y.factor11[j],",",y.factor12[j],"]") )
    par(mar=c(1,0,2,0))
    persp(copula.r1.r2, dMvdc, xlim = c(0, 1), ylim = c(0, 1), ticktype = "simple", xlab="r1", ylab="r1", main="Copula" )
    par(mar=c(2,2,5,0))
    plot(r1.r2, main="Scatterplot", xlab="r1", ylab="r2")
    hist <- hist(r1.r2[,2], xlab="r2", ylab="", ylim = c(0,N/3),main=paste0("r2 beta[",y.factor21[j],",",y.factor22[j],"]") )
    title(paste("Combination No. = ", j), outer=TRUE) 
  }},htmlfile = "Animation.html", img.name = "beta_no", title = "Animation", description = "Comparison of beta distributions" )



```

Column 
-----------------------------------------------------------------------



### Example 

``` {r echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
renderPlot({
  # Inputs from sliders
  N <- input$N
  tau <- input$tau
  alpha1 <- input$alpha1
  beta1 <- input$beta1
  alpha2 <- input$alpha2
  beta2 <- input$beta2
  
  # Define copula and generate data
  copula.r1.r2 <- mvdc(frankCopula(tau), 
                       margins = c("beta", "beta"), 
                       paramMargins = list(list(alpha1, beta1), list(alpha2, beta2)))
  r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))
  
  # Graphs
  par(mfrow = c(2, 2))
  
  # Histogram of r1
  hist(r1.r2[, 1], xlab = "r1", main = "Distribution of r1", col = "lightblue")
  
  # 3D Perspective of the PDF
  persp(copula.r1.r2, dMvdc, xlim = c(0, 1), ylim = c(0, 1), 
        xlab = "r1", ylab = "r2", zlab = "Density", main = "PDF", col = "lightgreen")
  
  # Scatterplot of r1 vs r2
  plot(r1.r2, xlab = "r1", ylab = "r2", main = "Scatterplot", col = "blue", pch = 19)
  
  # Histogram of r2
  hist(r1.r2[, 2], xlab = "r2", main = "Distribution of r2", col = "lightcoral")
  
  title("Frank Copula with Beta Marginals", outer = TRUE)
})
```




5. Profit and Correlation
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs:


```{r echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Inputs for Correlation Analysis
sliderInput("cost_param_corr", "Cost per Product (C) [PLN]", min = 50, max = 120, value = 90, step = 10)
sliderInput("price_min_corr", "Minimum Price (PLN)", min = 70, max = 90, value = 90, step = 10)
sliderInput("price_max_corr", "Maximum Price (PLN)", min = 90, max = 150, value = 120, step = 10)
sliderInput("sales_min_corr", "Minimum Units Sold", min = 90, max = 200, value = 100, step = 10)
sliderInput("sales_peak_corr", "Most Likely Units Sold", min = 200, max = 700, value = 350, step = 10)
sliderInput("sales_max_corr", "Maximum Units Sold", min = 700, max = 5000, value = 1000, step = 10)
sliderInput("damaged_rate_corr", "Damaged Packages (%)", min = 0, max = 20, value = 9, step = 1)
sliderInput("correlation_rho", "Correlation between Sales and Price (ρ)", min = -1, max = 1, value = 0, step = 0.1)
sliderInput("var_level_corr", "VaR Confidence Level (%)", min = 1, max = 10, value = 5, step = 1)  
  
sliderInput("iterations_corr", "Number of Simulations", min = 10000, max = 50000, value = 20000, step = 1000)

```

Column 
-----------------------------------------------------------------------


### Var + copula 
<h2>Correlation Analysis: Profitability Simulation</h2>

<h3>Overview</h3>
<p>
This simulation explores how the correlation between the sales quantity and the price of a product impacts the profitability and risk of a delivery business. By incorporating a Gaussian copula to model the dependence structure, the simulation provides insights into how positive, negative, or no correlation influences the profit distribution.
</p>

<h3>Key Variables</h3>
<ul>
  <li><strong>Cost per Product (C):</strong> The production cost for a single unit.</li>
  <li><strong>Sales:</strong> The number of units sold, modeled using a triangular distribution.</li>
  <li><strong>Prices:</strong> The selling price of a product, modeled using a uniform distribution.</li>
  <li><strong>Damaged Packages:</strong> A percentage of sold items that are damaged during delivery, incurring additional costs.</li>
  <li><strong>Correlation (ρ):</strong> The relationship between sales and prices, ranging from -1 (perfect negative correlation) to +1 (perfect positive correlation).</li>
</ul>

<h3>Simulation Workflow</h3>
<ol>
  <li>Generate correlated random variables using a Gaussian copula.</li>
  <li>Map these variables to the appropriate marginal distributions:
    <ul>
      <li><strong>Sales:</strong> Triangular distribution with specified minimum, maximum, and most likely values.</li>
      <li><strong>Prices:</strong> Uniform distribution within a defined range.</li>
    </ul>
  </li>
  <li>Calculate the revenue, total costs, and profit:
    <ul>
      <li><strong>Revenue:</strong> Based on sales quantity and prices, excluding damaged packages.</li>
      <li><strong>Total Costs:</strong> Sum of production costs and additional costs for damaged items.</li>
      <li><strong>Profit:</strong> Revenue minus total costs.</li>
    </ul>
  </li>
  <li>Analyze the impact of correlation on the profit distribution.</li>
</ol>

<h2>Impact of Correlation on Profitability and Risk</h2>

<h3>Positive Correlation</h3>
<p>
In scenarios where sales quantity and price are positively correlated, higher prices are associated with higher demand. This situation often reflects products perceived as luxury or high-quality items. Positive correlation can increase profitability but may also introduce risk due to potential overpricing in competitive markets.
</p>

<h3>Negative Correlation</h3>
<p>
Negative correlation indicates that higher prices lead to reduced demand, which is typical for most goods in competitive markets. While this reflects a standard demand-price relationship, it may limit profitability as the business faces reduced sales at higher price levels.
</p>

<h3>No Correlation</h3>
<p>
When there is no correlation between sales and prices, the demand is considered independent of pricing strategies. This randomness can lead to unpredictable profit outcomes, increasing the uncertainty and making risk assessment more challenging.
</p>

<h2>Insights and Practical Applications</h2>
<ul>
  <li><strong>Profitability:</strong> Positive correlation can enhance profitability but requires careful pricing strategies to avoid risks of demand loss.</li>
  <li><strong>Risk Management:</strong> Negative or no correlation introduces higher variability in profits, emphasizing the need for diversified revenue streams or hedging strategies.</li>
  <li><strong>Strategic Decision-Making:</strong> Understanding the correlation between sales and prices helps businesses optimize pricing strategies and mitigate risks.</li>
</ul>

<h3>Experimenting with Correlation</h3>
<p>
Users are encouraged to experiment with different correlation levels (\(ρ\)) to observe their impact on profit distribution. For example:
</p>
<ul>
  <li>Set \(ρ = 0.5\) to simulate a moderate positive correlation and evaluate its effect on profit and risk.</li>
  <li>Set \(ρ = -0.5\) to simulate a moderate negative correlation and analyze how lower prices impact demand and profitability.</li>
  <li>Set \(ρ = 0\) to simulate no correlation and observe the variability in profits.</li>
</ul>

Column 
-----------------------------------------------------------------------

### Model


```{r}

 renderPlot({
 
 # Parameters from inputs
  cost <- input$cost_param_corr
  price_min <- input$price_min_corr
  price_max <- input$price_max_corr
  sales_min <- input$sales_min_corr
  sales_peak <- input$sales_peak_corr
  sales_max <- input$sales_max_corr
  damaged_rate <- input$damaged_rate_corr / 100  # Convert percentage to proportion
  rho <- input$correlation_rho  # Correlation parameter
  iterations <- input$iterations_corr
  var_level <- input$var_level_corr/100
  
  
  # Define Gaussian copula with correlation rho
  copula_model <- normalCopula(param = rho, dim = 2, dispstr = "un")
  
  # Simulate correlated data using the copula
  copula_data <- rCopula(iterations, copula_model)
  
  # Map copula margins to appropriate distributions
  sales <- qtri(copula_data[, 1], min = sales_min, max = sales_max, mode = sales_peak) # Triangular for sales
  prices <- qunif(copula_data[, 2], min = price_min, max = price_max)                 # Uniform for prices
  
  # Calculating revenue, costs, and profit
  revenue <- (sales - damaged_rate * sales) * prices
  total_cost <- sales * cost + damaged_rate * sales * (cost + 10)  # Assume extra 10 PLN cost per damaged package
  profit <- revenue - total_cost
  
  # Calculate VaR
  VaR <- quantile(profit, probs = var_level)
  
  # Plotting profit distribution
  hist(profit, breaks = 50, col = "lightblue", border = "white",
       main = "Profit Distribution with VaR", xlab = "Profit (PLN)", ylab = "Frequency")
  abline(v = mean(profit), col = "red", lwd = 2, lty = 2)
  abline(v = VaR, col = "blue", lwd = 2, lty = 2)
  text(mean(profit), max(table(cut(profit, 50))) * 0.6, 
       paste("Mean Profit =", round(mean(profit), 0)), col = "red")
  text(VaR, max(table(cut(profit, 50))) * 0.7, 
       paste("VaR (", var_level, "%) =", round(VaR, 0)), col = "blue")
  
  title(sub = paste("Correlation (ρ) =", rho))
  
  
  # 
  #  # Plotting profit distribution
  # hist(profit, breaks = 50, col = "lightblue", border = "white",
  #      main = "Profit Distribution with Correlation", xlab = "Profit (PLN)", ylab = "Frequency")
  # abline(v = mean(profit), col = "red", lwd = 2, lty = 2)
  # text(mean(profit), max(table(cut(profit, 50))) * 0.9, 
  #      paste("Mean Profit =", round(mean(profit), 2)), col = "red")
  # 
  # # Show correlation on the plot
  # title(sub = paste("Correlation (ρ) =", rho))
})
```


5.Markowitz
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs: 

```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}

# Inputs
sliderInput("mean_A", "Mean of Asset A", min = -0.1, max = 0.1, value = 0.01, step = 0.01)
sliderInput("mean_B", "Mean of Asset B", min = -0.1, max = 0.1, value = -0.01, step = 0.01)
sliderInput("sd_A", "Std. Dev. of Asset A", min = 0.01, max = 2,  value = 0.1, step = 0.01)
sliderInput("sd_B", "Std. Dev. of Asset B", min = 0.01, max = 2, value = 0.1, step = 0.01)
sliderInput("correlation", "Correlation (via Copula)", min = -0.999, max = 0.999, value = 0, step = 0.01)

```

<p></p>


Column 
-----------------------------------------------------------------------
### Markowitz 

<h2> Markowitz Portfolio Theory </h2>

The simulation demonstrates the application of **Markowitz Portfolio Theory**, which is a foundational model in modern finance for constructing an optimal portfolio. This theory assumes that investors seek to maximize returns for a given level of risk or minimize risk for a given level of returns.

<h3> Key Elements in the Simulation </h3>

<h4>. Generation of Returns for Two Assets (A and B)</h4>
<ul>
<li>Returns for the two assets are simulated using a multivariate normal distribution.</li>
<li>The correlation between the assets is controlled by a copula, ensuring that the time series of returns exhibit the desired level of dependency.</li>
<li>The user can adjust the means, standard deviations, and correlation through sliders.</li>
</ul>

<h4>2. Mean-Variance Framework</h4>
<ul>
<li>The simulation calculates the mean and standard deviation of returns for both assets.</li>
<li>These are visualized as points labeled <strong>"A"</strong> and <strong>"B"</strong> in the mean-variance space.</li>
</ul>

<h4>3. Portfolio Combination</h4>
<ul>
<li>The simulation models portfolios consisting of varying proportions of Asset A and Asset B.</li>
<li>For a portfolio with weights <code>t</code> for Asset A and <code>1-t</code> for Asset B, the portfolio's:
  <ul>
    <li><strong>Mean return:</strong> 
    \[
    \mu_p = t \cdot \mu_A + (1-t) \cdot \mu_B
    \]</li>
    <li><strong>Variance:</strong>
    \[
    \sigma_p^2 = t^2 \cdot \sigma_A^2 + (1-t)^2 \cdot \sigma_B^2 + 2 \cdot t \cdot (1-t) \cdot \rho \cdot \sigma_A \cdot \sigma_B
    \]</li>
    <li><strong>Standard deviation:</strong>
    \[
    \sigma_p = \sqrt{\sigma_p^2}
    \]</li>
  </ul>
</li>
</ul>

<h3>Simulation Insights</h3>

<h4>1. Asset Diversification</h4>
<ul>
<li>By combining two assets with varying returns and risks, the portfolio's overall risk can be reduced through diversification.</li>
<li>The level of risk reduction depends on the correlation between the two assets.</li>
</ul>

<h4>2. Visualization of Trade-Offs</h4>
<ul>
<li>The simulation illustrates how shifting weights between Asset A and Asset B impacts the portfolio's risk and return.</li>
<li>Points <strong>"A"</strong> and <strong>"B"</strong> in the mean-variance space show individual asset characteristics, while the green line shows the trade-offs available to the investor.</li>
</ul>

<h4>3. Dynamic Parameters</h4>
<ul>
<li>Users can interactively adjust asset parameters (mean, standard deviation, correlation) to observe their impact on portfolio outcomes.</li>
</ul>

<h4>Practical Applications</h4>

<ul>
<li>The Markowitz model is widely used in investment management for constructing portfolios that align with an investor's risk tolerance.</li>
<li>The ability to visualize the efficient frontier aids in understanding the benefits of diversification and optimal portfolio selection.</li>
</ul>

Column 
-----------------------------------------------------------------------


### graphs 



```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}

# Simulation results
renderPlot({

N <- 20000

# rho<- -0.06 
# mean_A <- 0.05 
# sd_A  <- 0.09 
# mean_B <- - 0.02 
# sd_B  <- 0.09 


rho <- input$correlation # Correlation coefficient between Asset A and Asset B
mean_A <- input$mean_A   # Mean of returns for Asset A
sd_A  <- input$sd_A      # Standard deviation of returns for Asset A
mean_B <- input$mean_B   # Mean of returns for Asset B
sd_B  <- input$sd_B      # Standard deviation of returns for Asset B

# Define a multivariate distribution with a normal copula
copula.r1.r2 <- mvdc(normalCopula(rho, dim = 2), 
                     margins = c("norm", "norm"), # Marginal distributions are normal
                     paramMargins = list(list(mean_A, sd_A), list(mean_B, sd_B)))

# Generate data from the defined multivariate distribution
r1.r2 <- data.frame(rMvdc(N, copula.r1.r2))

# Graphing Section
par(mfrow = c(3, 2))      # Arrange plots in a 3x2 grid
par(mar = c(2, 2, 5, 0))  # Set margins for the first plot
# Histogram of the first variable (X, Asset A returns)
hist <- hist(r1.r2[, 1], xlab = "x", ylab = "", main = "Distribution of X", col = pal_col)

par(mar = c(1, 0, 2, 0))  # Adjust margins for the second plot
# Perspective plot of the joint density of the copula
persp(copula.r1.r2, dMvdc, ticktype = "simple", xlab = "X", ylab = "Y", 
      xlim = range(r1.r2[, 1]), ylim = range(r1.r2[, 2]), main = "H(X,Y)")

par(mar = c(2, 2, 5, 0))  # Adjust margins for the third plot
# Scatterplot of the two variables (Asset A and Asset B returns)

############################### function start ###################################
# dystribution of a and b as XY plot
plot_colorByDensity = function(x1,x2,
                               ylim=c(min(x2),max(x2)),
                               xlim=c(min(x1),max(x1)),
                               xlab="",ylab="",main="") {
  df <- data.frame(x1,x2)
  x <- densCols(x1,x2, colramp=colorRampPalette(c("black", "white")))
  df$dens <- col2rgb(x)[1,] + 1L
  cols <-  colorRampPalette(c("#000099", "#00FEFF", "#45FE4F","#FCFF00", "#FF9400", "#FF3100"))(256)
  df$col <- cols[df$dens]
  plot(x2~x1, data=df[order(df$dens),], 
       ylim=ylim,xlim=xlim,pch=20,col=col,
       cex=2,xlab=xlab,ylab=ylab,
       main=main)
}
############################### function end ###################################


plot_colorByDensity(r1.r2[,1],r1.r2[,2],
                    xlab="x",
                    ylab="y",
                    main = "Scatterplot")


#plot(r1.r2, main = "Scatterplot", xlab = "x", ylab = "y")

# Histogram of the second variable (Y, Asset B returns)
hist <- hist(r1.r2[, 2], xlab = "r2", ylab = "", main = "Distribution of y", col = pal_col)

# Portfolio weights
weights <- seq(0, 1, length.out = 100) # Generate a sequence of portfolio weights (0 to 1)

# Portfolio calculations
mean_portfolio <- weights * mean_A + (1 - weights) * mean_B # Calculate portfolio mean returns
var_portfolio <- weights^2 * sd_A^2 + (1 - weights)^2 * sd_B^2 + 
                 2 * weights * (1 - weights) * rho * sd_A * sd_B # Portfolio variance
sd_portfolio <- sqrt(var_portfolio) # Portfolio standard deviation (risk)

par(mar = c(4, 4, 3, 3)) # Set margins for the plot
# Plot the mean-variance space for the portfolio
plot(sd_portfolio, mean_portfolio, type = "l", 
     ylim = c(min(mean_A, mean_B) - 0.01, max(mean_A, mean_B)),
     col = "green4", 
     lwd = 2,
     main = "Mean-Variance Space", 
     xlab = "Standard Deviation", 
     ylab = "Mean Return")

# Add points for individual assets in the mean-variance space
points(sd_A, mean_A, col = "blue4", pch = 19, cex = 1.2) # Point for Asset A
text(sd_A ,mean_A,  labels = "A", pos = 2, col = "blue1", cex = 1.5) # Label for Asset A
points(sd_B, mean_B, col = "red", pch = 19, cex = 1.2)  # Point for Asset B
text(sd_B, mean_B, labels = "B", pos = 2, col = "red3", cex = 1.5)  # Label for Asset B

# Plot portfolio risk (standard deviation) vs. weights
plot(weights, sd_portfolio, type = "l", col = "green4", lwd = 2,
     main = "Portfolio Risk vs. Weights", xlab = "t - Weights of Asset A ", ylab = "Standard Deviation")
  })


```


5.VAR GMB
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs: 

```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}

# Sliders for user input
sliderInput("mu", "Drift (Trend) - μ", min = -0.05, max = 0.05, value = 0.01, step = 0.01)
sliderInput("sigma", "Volatility - σ", min = 0, max = 0.5, value = 0.05, step = 0.01)
sliderInput("initial_price", "Initial Price (S₀) [in million PLN]", min = 0.4, max = 1, value = 0.54, step = 0.01)
sliderInput("time_horizon", "Time Horizon (Months)", min = 12, max = 60, value = 24, step = 6)
sliderInput("iterations", "Number of Simulations", min = 500, max = 10000, value = 1000, step = 1000)

```

<p></p>


Column 
-----------------------------------------------------------------------

### Geometric Brownian Motion

<h2>Value at Risk</h2>

<p>
Sometimes we are pessimistic that our model is not sufficient to predict correctly, as the time series might appear to follow a purely random process. In such cases, we aim to determine the level of risk we are exposed to. For this purpose, we can use <strong>Brownian Geometric Motion</strong>.
</p>

<h3>Geometric Brownian Motion</h3>
<p>
Many phenomena in economics, such as stock prices or exchange rates, are simulated using Geometric Brownian Motion (GBM). This model, while not perfect (e.g., in the Black-Scholes framework), provides a reasonable approximation for price behavior over time. The price process satisfies the stochastic differential equation:
</p>

<p style="text-align:center;">
\[
dS_t = \mu S_t dt + \sigma S_t dB_t
\]
</p>

<p>
Where:
<ul>
  <li>\( S_t \): The price at time \( t \)</li>
  <li>\( \mu \): Price drift (trend), representing the average change</li>
  <li>\( \sigma \): Price volatility (standard deviation), representing uncertainty</li>
  <li>\( dB_t \): A random Brownian motion term</li>
</ul>
</p>

<p>
The formula used to simulate price paths via Geometric Brownian Motion is as follows:
</p>

<p style="text-align:center;">
\[
S_t = S_{t-1} \exp\left[\left(\mu - \frac{1}{2} \sigma^2\right)dt + \sigma \sqrt{dt} \varepsilon_i \right]
\]
</p>

<p>
Where:
<ul>
  <li>\( \varepsilon_i \): Random variable with \( N(0,1) \) distribution</li>
  <li>\( dt = \frac{T}{I} \): Time step for the simulation, where \( T \) is the total time horizon and \( I \) is the number of intervals</li>
</ul>
</p>

<h3>Simulating Future Prices and VaR</h3>
<p>
To estimate future apartment prices, we use the GBM model with parameters derived from historical data. The initial price (\( S_0 \)) is set at 0.54 million PLN. Based on the historical monthly changes, the following values are used:
</p>
<ul>
  <li>\( \sigma = 0.055 \): Volatility of price changes</li>
  <li>\( \mu = 0.0023 \): Estimated drift (trend)</li>
  <li>\( dt = 0.05 \): Time step in years</li>
</ul>

<p>
We simulate 10,000 trajectories of price paths over the next 12 months (\( t = 10 \)) and 24 months (\( t = 20 \)). For each horizon, we calculate the <strong>Value at Risk (VaR)</strong>, defined as the 5th percentile of the simulated price distribution.
</p>

<h3>Interactive Exploration</h3>
<p>
To better understand how market dynamics affect the apartment price, we perform sensitivity analysis by varying key parameters:
</p>

<h4>1. Drift (\( \mu \))</h4>
<p>
The drift represents the average monthly price increase. By adjusting \( \mu \) from 0 to 0.01, we explore different growth scenarios, from stagnant markets to rapidly appreciating prices.
</p>

<h4>2. Volatility (\( \sigma \))</h4>
<p>
Volatility captures the uncertainty in price changes. By varying \( \sigma \) from 0 to 0.01, we analyze the impact of risk on potential price outcomes and VaR.
</p>

<h4>Simulation Outcomes</h4>
<p>
The simulation results provide the following insights:
<ul>
  <li>Projected price ranges at 12 and 24 months, including most likely outcomes</li>
  <li>Impact of changes in drift (\( \mu \)) and volatility (\( \sigma \)) on future prices</li>
  <li>Value at Risk (VaR), quantifying the worst-case price drop with 95% confidence</li>
</ul>
</p>

<h3>Conclusion</h3>
<p>
By simulating future prices using Geometric Brownian Motion, we gain insights into the potential range of price changes and their associated risks. This information is crucial for both investors and risk managers to make informed decisions regarding real estate investments.
</p>

Column {.tabset}
-------------------------------------

### grph 1 

```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}
renderPlot({
  # Parameters from sliders
  mu <- input$mu
  sigma <- input$sigma
  S0 <- input$initial_price
  T <- input$time_horizon / 12  # Convert months to years
  I <- input$time_horizon  # Number of time steps
  K <- input$iterations
  dt <- T / I  # Time step size
  
  # Simulate price trajectories
  zeta <- matrix(rnorm(K * I, 0, 1), nrow = I, ncol = K)
  S <- matrix(nrow = I, ncol = K)
  S[1, ] <- S0
  
  for (j in 1:K) {
    for (i in 2:I) {
      S[i, j] <- S[i - 1, j] * exp((mu - (sigma^2) / 2) * dt + sigma * sqrt(dt) * zeta[i, j])
    }
  }
  
  t <- input$time_horizon 
  
  
  # Plot simulated price trajectories
  matplot(S, type = "l", lty = 1, main = "Simulated Price Trajectories", 
          xlab = "Time Steps", ylab = "Price (Million PLN)")
  abline(v = t, col = "red", lty = 2)  # Mark time horizon t months
  
})
```


### grap 2 

```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}
renderPlot({
  
  # Parameters from sliders
  mu <- input$mu
  sigma <- input$sigma
  S0 <- input$initial_price
  T <- input$time_horizon / 12  # Convert months to years
  I <- input$time_horizon  # Number of time steps
  K <- input$iterations
  dt <- T / I  # Time step size
  
  # Simulate price trajectories
  zeta <- matrix(rnorm(K * I, 0, 1), nrow = I, ncol = K)
  S <- matrix(nrow = I, ncol = K)
  S[1, ] <- S0
  
  for (j in 1:K) {
    for (i in 2:I) {
      S[i, j] <- S[i - 1, j] * exp((mu - (sigma^2) / 2) * dt + sigma * sqrt(dt) * zeta[i, j])
    }
  }
 

par(mfrow=c(1,1))
t <- input$time_horizon
main_tex <- paste0("Price, VaR = ",round(quantile((S[t,]), probs=c(5)/100),2) ," for t = ", t  )
hist( S[t,], 
      prob=TRUE, 
      main= main_tex , 
      xlab = "10^6 PLN", 
      breaks = 20, 
      col = wes_palette(n=6,type = "continuous", name="Royal1")[1])

lines(density(S[t,], na.rm = TRUE))
abline(v = quantile(S[t,], probs=c(5)/100), col = "red") 
text(round(quantile((S[t,]), probs=c(5)/100),2), 3, labels = "VaR", pos = 1, col = "red", cex = 1.2)  # Label for Asset B
abline(v = mean(S[t,]), col = "green") 
text(round(mean((S[t,]), probs=c(5)/100),2), 3, labels = "Mean", pos = 1, col = "green", cex = 1.2)  # Label for Asset B



})
```


5.Boot(a)
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------

Inputs: 

```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}

# Slider for the number of bootstrap iterations
sliderInput("bootstrap_iterations", "Number of Bootstrap Iterations", 
            min = 100, max = 5000, value = 1000, step = 100)

```

<p></p>

Column 
-----------------------------------------------------------------------

### Intro



<h2>Bootstrap: A Powerful Resampling Technique</h2>

<p>Bootstrapping is a statistical method used to estimate the variability and distribution of a statistic by repeatedly resampling the original data with replacement. Unlike traditional parametric methods, which rely on strong assumptions about the underlying data distribution, bootstrap methods are nonparametric and do not require prior knowledge of the distribution. This makes bootstrapping highly versatile and applicable to a wide range of problems, including regression analysis, hypothesis testing, and confidence interval estimation.</p>

<h3>How Does Bootstrapping Work?</h3>

<ul>
  <li><strong>Step 1:</strong> Start with an observed dataset containing \( n \) samples.</li>
  <li><strong>Step 2:</strong> Generate a "bootstrap sample" by randomly selecting \( n \) data points from the original dataset, allowing for replacement (some points may be selected multiple times, while others may be excluded).</li>
  <li><strong>Step 3:</strong> Calculate the statistic of interest (e.g., mean, variance, regression coefficients) on the bootstrap sample.</li>
  <li><strong>Step 4:</strong> Repeat steps 2 and 3 a large number of times (e.g., \( B = 1000 \)) to obtain a distribution of the statistic.</li>
  <li><strong>Step 5:</strong> Analyze the bootstrap distribution to estimate measures like standard errors, confidence intervals, or variability of the statistic.</li>
</ul>

<p>Bootstrapping leverages the principle that the observed data provides a good approximation of the population distribution. By resampling with replacement, we mimic the process of drawing multiple samples from the true population.</p>

<h3>Simulation Objective</h3>

<p>The goal of this simulation is to estimate the variability of the regression coefficients \( \alpha \) (intercept) and \( \beta \) (slope) in the linear regression model:</p>

\[
y = \alpha + \beta x + \epsilon
\]

<p>Here, \( y \) represents the dependent variable, \( x \) is the independent variable, and \( \epsilon \) is the error term. We begin with a dataset containing 350 observations and fit a linear regression model to estimate \( \alpha \) and \( \beta \). Using bootstrapping, we generate multiple resampled datasets to observe the variability in these parameter estimates.</p>


<h3>Bootstrap Simulation Details</h3>

<h4>Original Sample Data</h4>

<p>The simulation starts by visualizing the original sample data and fitting a linear regression model. This provides the initial estimates of \( \alpha \) and \( \beta \), which serve as a benchmark for comparison with the bootstrap results.</p>

<ul>
  <li>The original dataset contains 350 observations of variables \( x \) and \( y \).</li>
  <li>A linear regression model is fitted to estimate \( y \) as a function of \( x \).</li>
</ul>

<p>The regression equation is of the form:</p>

\[
y = \alpha + \beta x
\]

<p>The scatterplot of the data is shown alongside the fitted regression line, allowing us to visually assess the fit of the model.</p>

<h3>Bootstrap Procedure</h3>

<p>The bootstrap procedure follows these steps:</p>

<ul>
  <li>Generate \( B \) bootstrap samples by resampling the observed data with replacement.</li>
  <li>For each bootstrap sample:
    <ul>
      <li>Fit a linear regression model to the resampled data.</li>
      <li>Store the estimated \( \alpha \) and \( \beta \) coefficients.</li>
    </ul>
  </li>
  <li>After \( B \) iterations, analyze the distributions of \( \alpha \) and \( \beta \) to assess their variability.</li>
</ul>

<p>The histograms of \( \alpha \) and \( \beta \) show the spread of these estimates across all bootstrap samples, providing insights into their variability. Additionally, the joint distribution of \( \alpha \) and \( \beta \) is visualized in a scatterplot, colored by density.</p>


<h3>Interactive Features</h3>

<p>This simulation includes an interactive slider that allows users to adjust the number of bootstrap iterations. Increasing the number of iterations improves the accuracy of the bootstrap distribution but comes at the cost of higher computational effort. The visualizations update dynamically based on the selected number of iterations, enabling users to explore the impact of bootstrap sample size on the results.</p>

<h3>Key Insights</h3>

<p>The bootstrap simulation provides several important insights:</p>

<ul>
  <li><strong>Variability of Coefficients:</strong> The spread of the bootstrap distributions for \( \alpha \) and \( \beta \) quantifies the uncertainty in these parameter estimates.</li>
  <li><strong>Confidence Intervals:</strong> The bootstrap results can be used to calculate confidence intervals for \( \alpha \) and \( \beta \), which provide a range of plausible values for these parameters.</li>
  <li><strong>Robustness:</strong> Bootstrapping is robust to violations of normality and other assumptions, making it a powerful tool for regression analysis and other statistical tasks.</li>
</ul>

<p>This method highlights the importance of assessing variability and uncertainty in statistical estimates, particularly in small sample sizes or when the underlying data distribution is unknown.</p>

<h3>Conclusion</h3>

<p>Bootstrap methods offer a flexible and intuitive approach to understanding the variability of model estimates. In this simulation, the bootstrapped distributions of the regression coefficients \( \alpha \) and \( \beta \) provide valuable insights into the reliability and uncertainty of the model parameters.</p>

<h2>Warning: Interdependence of \( \alpha \) and \( \beta \) in Linear Models</h2>

<p>In a linear regression model of the form:</p>

\[
y = \alpha + \beta x + \epsilon
\]

<p>the coefficients \( \alpha \) (intercept) and \( \beta \) (slope) are not independent. Simultaneous estimation based on sample means can introduce bias and unreliable parameter estimates, especially when outliers or non-linear relationships exist. The variability in \( \alpha \) and \( \beta \) is intertwined due to their shared dependency on the mean structure of the data.</p>

<h3>Better Alternatives for Robust Parameter Estimation</h3>

<p>To address this problem, methods such as **PAM (Partitioning Around Medoids)** or **K-means clustering** can be employed to partition the data into subsets, reducing the influence of extreme values. These methods divide the dataset into clusters and then perform regression within each cluster:</p>

<ul>
  <li><strong>PAM:</strong> A robust clustering method that minimizes the impact of outliers by selecting cluster centers (medoids) from the data.</li>
  <li><strong>K-means:</strong> A simpler clustering approach that minimizes the total distance between observations and cluster centroids.</li>
</ul>

<p>By clustering data first, we reduce the impact of interdependence in estimating \( \alpha \) and \( \beta \), leading to more robust regression results.</p>


Column 
-----------------------------------------------------------------------




### graph 

```{r}
renderPlot({

sample_2023 <- read_csv("sample_2023.csv") # load the sample 

par(mfrow=c(2,2)) #to have two graphs in one window one above the other


# the estimation of based on the sample of 350 observations
mnk <- lm(sample_2023$y~sample_2023$x)
mnk
#estimated coefficients
# summary(mnk)$coefficients[,1][1] extraction of coefficient number 
alfa <- round(summary(mnk)$coefficients[,1][1], digits = 0 )  #rounding coefficient
beta <- round(summary(mnk)$coefficients[,1][2], digits = 3 )  
#scatterplot 
plot(sample_2023$x, sample_2023$y, pch=19, 
     main = paste0("Estimation based on sample y(t) = ", alfa," + ",beta,"*x(t)" ))
abline(h=(-20:25)*2000, lty=3) #adding lines in the background (horizontal)
abline(v=(-20:25)*2000, lty=3) #adding lines in the background (vertical)
abline(mnk) #adding trend line from regression

  
# START the bootsrap procedure
# bootstrap 
# A. discussion of the function sample 
#takes a sample of the specified size from the elements of x using either with or without replacement.
#arguments of the function:
#1) 1:nrow(sample_2023) (350) - a set from which we are drawing elements  
#2) nrow(sample_2023) - number of elements to choose to a new sample
#3) should sampling be with replacement? replace=TRUE yes with replacement 
sample_b_ind_ewa <-sample(1:nrow(sample_2023), nrow(sample_2023), replace=TRUE)
sample_b_ind_ewa # a vector consisting of 350 elements index of each row in sample all 
table(sample_b_ind_ewa)
# B. we need to draw randomly with replacement a set of observations from sample_2023
#1. select randomly with replacement an index rows with function (sample)
#2. then subset sample_2023 to the rows selected by function (sample sample_b_ind)
sample_b_ewa<-sample_2023[sample_b_ind_ewa,] 
# sample_b - choosing row indexes from sample all 
#3. perform regression on sample_b 
#4. store results of a regression to a vector alfa_b and beta_b
# repeat the process 1000 times 

#repeat the process 1000
# input object to the loop epmty vectors before a loop to store obtained results 
#(like an empty page to store results)
sample_b_ind  <-0
alfa_b <- 0
beta_b <- 0
B <- input$bootstrap_iterations# Bootsprap iterations
for(i in 1:B){
  sample_b_ind <-sample(1:nrow(sample_2023), nrow(sample_2023), replace=TRUE) #label draw
  sample_b <- sample_2023[sample_b_ind,] 
  mnk_b <- lm(sample_b$y~sample_b$x)
  
  alfa_b[i] <- summary(mnk_b)$coefficients[,1][1]  
  beta_b[i] <- summary(mnk_b)$coefficients[,1][2]  }
# END  
alfa_b 
beta_b 

# distribution of a and b estimates

#histogram for alfa_a
hist(alfa_b, main="Histogram of a", col="aquamarine3", breaks=50, probability=TRUE)
lines(density(alfa_b), lwd=3)
#summary of coefficient alfa_a
summary(alfa_b)
#histogram for alfa_b
hist(beta_b, main="Histogram of b", col="aquamarine3", breaks=50, probability=TRUE)
lines(density(beta_b), lwd=3)
#summary of coefficient alfa_b
summary(beta_b)


############################### function start ###################################
# distribution of a and b as XY plot
#creating a function that will help to make a graph (scatterplot of x and y estimates of 1000 repetitions)
#densCols produces a vector containing colors which encode the local densities at each point in a scatterplot.
plot_colorByDensity = function(x1,x2,
                               ylim=c(min(x2),max(x2)),
                               xlim=c(min(x1),max(x1)),
                               xlab="",ylab="",main="") {
  df <- data.frame(x1,x2)
  x <- densCols(x1,x2, colramp=colorRampPalette(c("black", "white")))
  df$dens <- col2rgb(x)[1,] + 1L
  cols <-  colorRampPalette(c("#000099", "#00FEFF", "#45FE4F","#FCFF00", "#FF9400", "#FF3100"))(256)
  df$col <- cols[df$dens]
  plot(x2~x1, data=df[order(df$dens),], 
       ylim=ylim,xlim=xlim,pch=20,col=col,
       cex=2,xlab=xlab,ylab=ylab,
       main=main)}
############################### function end ###################################
#using the above function on our data
plot_colorByDensity(alfa_b,beta_b,xlab="a",ylab="b",
                    main = paste0("XY plot of bootstrapped estimators \n [mean_a =", round( mean(alfa_b),1), ", mean_b =", round( mean(beta_b),2), "]"))
abline(h=mean(beta_b), v=mean(alfa_b), lwd=2)
  
})


```


6.Boot(b)
=======================================================================

Inputs {.sidebar}
-----------------------------------------------------------------------
<p></p>
Inputs: 

```{r echo = FALSE, warning = FALSE, error=FALSE, message=FALSE}

sliderInput("num_trees", "Number of Trees (Random Forest)", 
            min = 10, max = 500, value = 100, step = 10)
sliderInput("train_fraction", "Training Set Fraction", 
            min = 0.5, max = 0.9, value = 0.7, step = 0.1)

```



Column 
-----------------------------------------------------------------------

### Intro

<h2>Bootstrap in Machine Learning: Random Forest</h2>

<p>Bootstrap resampling plays a critical role in reducing the variance of machine learning models, particularly in ensemble methods such as <strong>Random Forest</strong>. Unlike a single decision tree, which can overfit the training data and produce high variance, Random Forest aggregates results from multiple trees built on bootstrap samples of the training data.</p>

<h3>How Random Forest Works</h3>
<ul>
  <li>Bootstrap sampling creates multiple subsets of the training data (with replacement).</li>
  <li>Each decision tree is trained on a bootstrap sample, introducing diversity in the model.</li>
  <li>The final prediction is made by averaging (regression) or voting (classification) the outputs of individual trees.</li>
</ul>

<p>The aggregation step reduces model variance without significantly increasing bias, making Random Forest more robust and accurate compared to a single decision tree.</p>

<h3>Variance Reduction and Performance Improvement</h3>
<p>By using bootstrap sampling, Random Forest leverages the following advantages:</p>
<ul>
  <li><strong>Variance Reduction:</strong> Combining multiple trees smooths out random fluctuations in the data.</li>
  <li><strong>Improved Generalization:</strong> Random Forest performs well on unseen data by avoiding overfitting.</li>
  <li><strong>Feature Importance:</strong> Bootstrap allows the model to evaluate the relative importance of input features.</li>
</ul>

<h3>Simulation: Decision Tree vs. Random Forest</h3>
<p>In this example, we compare the performance of a single decision tree and a Random Forest model trained on the <code>mtcars</code> dataset. The target variable is the car's fuel efficiency (<code>mpg</code>), and the predictors include car characteristics such as horsepower, weight, and displacement.</p>

<ul>
  <li>Adjust the number of trees in the Random Forest using the slider.</li>
  <li>Compare the performance of both models using the Root Mean Squared Error (RMSE).</li>
</ul>

<p>Random Forest consistently outperforms a single decision tree by producing lower RMSE values and better generalization on the test set.</p>

<h3>Extensions to Ensemble Learning</h3>
<p>The bootstrap-based approach used in Random Forest can be extended to other ensemble methods such as:</p>
<ul>
  <li><strong>Bagging:</strong> Similar to Random Forest but without introducing random feature selection.</li>
  <li><strong>Boosting:</strong> Sequentially trains models to focus on difficult observations by adjusting weights.</li>
  <li><strong>Stacking:</strong> Combines predictions from multiple models using another model as a meta-learner.</li>
</ul>

<p>These methods leverage the power of bootstrap to create diverse models and aggregate their predictions, improving accuracy and reducing overfitting.</p>



Column 
-----------------------------------------------------------------------

### graph

```{r}
renderPlot({
  # Data Preparation
  set.seed(123)
  data <- mtcars
  train_index <- sample(1:nrow(data), size = input$train_fraction * nrow(data))
  train <- data[train_index, ]
  test <- data[-train_index, ]
  
  # Decision Tree Model
  tree_model <- rpart(mpg ~ ., data = train)
  tree_pred <- predict(tree_model, test)
  tree_rmse <- sqrt(mean((test$mpg - tree_pred)^2))
  
  # Random Forest Model with Bootstrap
  rf_model <- randomForest(mpg ~ ., data = train, ntree = input$num_trees)
  rf_pred <- predict(rf_model, test)
  rf_rmse <- sqrt(mean((test$mpg - rf_pred)^2))
  
  # Visualization
  par(mfrow = c(1, 2))
  # Decision Tree Results
  plot(test$mpg, tree_pred, main = "Decision Tree",
       xlab = "Actual MPG", ylab = "Predicted MPG", col = "blue", pch = 19)
  abline(0, 1, col = "red", lwd = 2)
  legend("bottomright", legend = paste("RMSE =", round(tree_rmse, 2)), bty = "n")
  
  # Random Forest Results
  plot(test$mpg, rf_pred, main = "Random Forest",
       xlab = "Actual MPG", ylab = "Predicted MPG", col = "green", pch = 19)
  abline(0, 1, col = "red", lwd = 2)
  legend("bottomright", legend = paste("RMSE =", round(rf_rmse, 2)), bty = "n")
  
  # Title
  mtext("Comparison of Decision Tree and Random Forest", outer = TRUE, cex = 1.5)
})




```













